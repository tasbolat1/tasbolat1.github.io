<!DOCTYPE html> <html lang="en"> <head> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title>publications | Tasbolat Taunyazov</title> <meta name="author" content="Tasbolat Taunyazov"/> <meta name="description" content="A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. "/> <meta name="keywords" content="tasbolat, taunyazov, robotics, tactile reasoning, artificial intelligence"/> <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha256-DF7Zhf293AJxJNTmh5zhoYYIMs2oXitRfBjY+9L//AY=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"/> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.4/css/all.min.css" integrity="sha256-mUZM63G8m73Mcidfrv5E+Y61y7a12O5mW4ezU3bxqW4=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/academicons@1.9.1/css/academicons.min.css" integrity="sha256-i1+4qU2G2860dGGIOJscdC30s9beBXjFfzjWLjBRsBg=" crossorigin="anonymous"> <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/github.css" media="none" id="highlight_theme_light"/> <link rel="shortcut icon" href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22>⚛️</text></svg>"> <link rel="stylesheet" href="/assets/css/main.css"> <link rel="canonical" href="https://tasbolat1.github.io/publications/"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/native.css" media="none" id="highlight_theme_dark"/> <script src="/assets/js/theme.js"></script> <script src="/assets/js/dark_mode.js"></script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="https://tasbolat1.github.io/"><span class="font-weight-bold">Tasbolat</span> Taunyazov</a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about</a> </li> <li class="nav-item "> <a class="nav-link" href="/news/">news</a> </li> <li class="nav-item active"> <a class="nav-link" href="/publications/">publications<span class="sr-only">(current)</span></a> </li> <li class="nav-item "> <a class="nav-link" href="/projects/">research</a> </li> <li class="nav-item "> <a class="nav-link" href="/assets/pdf/CV.pdf" target="_blank">cv</a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fas fa-moon"></i> <i class="fas fa-sun"></i> </button> </li> </ul> </div> </div> </nav> </header> <div class="container mt-5"> <div class="post"> <header class="post-header"> <h1 class="post-title">publications</h1> <p class="post-description"></p> </header> <article> <div class="publications"> <h2 class="year">2024</h2> <ol class="bibliography"><li> <div class="row"> <div class="col-sm-2 abbr"> <abbr class="badge">R:SS</abbr> </div> <div id="taunyazov2023grace" class="col-sm-8"> <div class="title">GRaCE: Balancing Multiple Criteria to Achieve Stable, Safe, and Functional Grasps</div> <div class="author"> <em>Taunyazov, Tasbolat</em>, Lin, Kelvin,  and <a href="https://haroldsoh.github.io/" target="_blank" rel="noopener noreferrer">Soh, Harold</a> </div> <div class="periodical"> <em>Robotics: Science and Systems (R:SS)</em> 2024 </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="http://arxiv.org/abs/2309.08887" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">arXiv</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://clear-nus.github.io/blog/grace" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Blog</a> <a href="https://github.com/clear-nus/grace" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Code</a> </div> <div class="abstract hidden"> <p>This paper addresses the multi-faceted problem of robot grasping, where multiple criteria may conflict and differ in importance. We introduce Grasp Ranking and Criteria Evaluation (GRaCE), a novel approach that employs hierarchical rule-based logic and a rank-preserving utility function to optimize grasps based on various criteria such as stability, kinematic constraints, and goal-oriented functionalities. Additionally, we propose GRaCE-OPT, a hybrid optimization strategy that combines gradient-based and gradient-free methods to effectively navigate the complex, non-convex utility function. Experimental results in both simulated and real-world scenarios show that GRaCE requires fewer samples to achieve comparable or superior performance relative to existing methods. The modular architecture of GRaCE allows for easy customization and adaptation to specific application needs.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">taunyazov2023grace</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{GRaCE: Balancing Multiple Criteria to Achieve Stable, Safe, and Functional Grasps}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Taunyazov, Tasbolat and Lin, Kelvin and Soh, Harold}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2024}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{Robotics: Science and Systems (R:SS)}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li></ol> <h2 class="year">2023</h2> <ol class="bibliography"><li> <div class="row"> <div class="col-sm-2 abbr"> <abbr class="badge">IROS</abbr> </div> <div id="taunyazov2023refining" class="col-sm-8"> <div class="title">Refining 6-DoF Grasps with Context-Specific Classifiers</div> <div class="author"> <em>Taunyazov, Tasbolat</em>, Zhang, Heng, Eala, John Patrick, Zhao, Na,  and <a href="https://haroldsoh.github.io/" target="_blank" rel="noopener noreferrer">Soh, Harold</a> </div> <div class="periodical"> <em>IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)</em> 2023 </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="http://arxiv.org/abs/2308.06928" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">arXiv</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://clear-nus.github.io/blog/graspflow" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Blog</a> <a href="https://github.com/clear-nus/graspflow" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Code</a> </div> <div class="abstract hidden"> <p>In this work, we present GraspFlow, a refinement approach for generating context-specific grasps. We formulate the problem of grasp synthesis as a sampling problem: we seek to sample from a context-conditioned probability distribution of successful grasps. However, this target distribution is unknown. As a solution, we devise a discriminator gradient-flow method to evolve grasps obtained from a simpler distribution in a manner that mimics sampling from the desired target distribution. Unlike existing approaches, GraspFlow is modular, allowing grasps that satisfy multiple criteria to be obtained simply by incorporating the relevant discriminators. It is also simple to implement, requiring minimal code given existing auto-differentiation libraries and suitable discriminators. Experiments show that GraspFlow generates stable and executable grasps on a real-world Panda robot for a diverse range of objects. In particular, in 60 trials on 20 different household objects, the first attempted grasp was successful 94% of the time, and 100% grasp success was achieved by the second grasp. Moreover, incorporating a functional discriminator for robot-human handover improved the functional aspect of the grasp by up to 33%.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">taunyazov2023refining</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Refining 6-DoF Grasps with Context-Specific Classifiers}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Taunyazov, Tasbolat and Zhang, Heng and Eala, John Patrick and Zhao, Na and Soh, Harold}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2023}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li></ol> <h2 class="year">2021</h2> <ol class="bibliography"><li> <div class="row"> <div class="col-sm-2 abbr"> <abbr class="badge">IROS</abbr> <span class="award_new badge">Best Paper</span> </div> <div id="taunyazov2021extended" class="col-sm-8"> <div class="title">Extended Tactile Perception: Vibration Sensing through Tools and Grasped Objects</div> <div class="author"> <em>Taunyazov, Tasbolat</em>, Song, Luar Shui, Lim, Eugene, See, Hian Hian, Lee, David, <a href="https://en.wikipedia.org/wiki/Benjamin_Tee" target="_blank" rel="noopener noreferrer">Tee, Benjamin CK</a>,  and <a href="https://haroldsoh.github.io/" target="_blank" rel="noopener noreferrer">Soh, Harold</a> </div> <div class="periodical"> <em>In IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)</em> 2021 </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="http://arxiv.org/abs/2106.00489" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">arXiv</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://github.com/clear-nus/ext-sense.git" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Code</a> <a href="/assets/pdf/presentation_IROS2021.pdf" class="btn btn-sm z-depth-0" role="button">Slides</a> </div> <div class="abstract hidden"> <p>Humans display the remarkable ability to sense the world through tools and other held objects. For example, we are able to pinpoint impact locations on a held rod and tell apart different textures using a rigid probe. In this work, we consider how we can enable robots to have a similar capacity, i.e., to embody tools and extend perception using standard grasped objects. We propose that vibro-tactile sensing using dynamic tactile sensors on the robot fingers, along with machine learning models, enables robots to decipher contact information that is transmitted as vibrations along rigid objects. This paper reports on extensive experiments using the BioTac micro-vibration sensor and a new event dynamic sensor, the NUSkin, capable of multi- taxel sensing at 4 kHz. We demonstrate that fine localization on a held rod is possible using our approach (with errors less than 1 cm on a 20 cm rod). Next, we show that vibro-tactile perception can lead to reasonable grasp stability prediction during object handover, and accurate food identification using a standard fork. We find that multi-taxel vibro-tactile sensing at a sufficiently high sampling rate led to the best performance across the various tasks and objects. Taken together, our results provide both evidence and guidelines for using vibro-tactile perception to extend tactile perception, which we believe will lead to enhanced competency with tools and better physical human-robot interaction.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">taunyazov2021extended</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Extended Tactile Perception: Vibration Sensing through Tools and Grasped Objects}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Taunyazov, Tasbolat and Song, Luar Shui and Lim, Eugene and See, Hian Hian and Lee, David and Tee, Benjamin CK and Soh, Harold}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{1755--1762}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2021}</span><span class="p">,</span>
  <span class="na">organization</span> <span class="p">=</span> <span class="s">{IEEE}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li></ol> <h2 class="year">2020</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col-sm-2 abbr"> <abbr class="badge">R:SS</abbr> </div> <div id="taunyazov2020event" class="col-sm-8"> <div class="title">Event-driven visual-tactile sensing and learning for robots</div> <div class="author"> <em>Taunyazov, Tasbolat</em>, Sng, Weicong, See, Hian Hian, Lim, Brian, Kuan, Jethro, Ansari, Abdul Fatir, <a href="https://en.wikipedia.org/wiki/Benjamin_Tee" target="_blank" rel="noopener noreferrer">Tee, Benjamin CK</a>,  and <a href="https://haroldsoh.github.io/" target="_blank" rel="noopener noreferrer">Soh, Harold</a> </div> <div class="periodical"> <em>Robotics: Science and Systems (R:SS)</em> 2020 </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="http://arxiv.org/abs/2009.07083" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">arXiv</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://clear-nus.github.io/visuotactile/" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Blog</a> <a href="https://github.com/clear-nus/VT_SNN" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Code</a> <a href="/assets/pdf/RSS_presentation.pdf" class="btn btn-sm z-depth-0" role="button">Slides</a> </div> <div class="abstract hidden"> <p>This work contributes an event-driven visual-tactile perception system, comprising a novel biologically-inspired tactile sensor and multi-modal spike-based learning. Our neuromorphic fingertip tactile sensor, NeuTouch, scales well with the number of taxels thanks to its event-based nature. Likewise, our Visual- Tactile Spiking Neural Network (VT-SNN) enables fast perception when coupled with event sensors. We evaluate our visual-tactile system (using the NeuTouch and Prophesee event camera) on two robot tasks: container classification and rotational slip detection. On both tasks, we observe good accuracies relative to standard deep learning methods. We have made our visual-tactile datasets freely-available to encourage research on multi-modal event-driven robot perception, which we believe is a promising approach towards intelligent power-efficient robot systems.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">taunyazov2020event</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Event-driven visual-tactile sensing and learning for robots}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Taunyazov, Tasbolat and Sng, Weicong and See, Hian Hian and Lim, Brian and Kuan, Jethro and Ansari, Abdul Fatir and Tee, Benjamin CK and Soh, Harold}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{Robotics: Science and Systems (R:SS)}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2020}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"> <abbr class="journal badge">Sensors</abbr> </div> <div id="shintemirov2020open" class="col-sm-8"> <div class="title">An open-source 7-DOF wireless human arm motion-tracking system for use in robotics research</div> <div class="author"> <a href="https://www.alaris.kz/" target="_blank" rel="noopener noreferrer">Shintemirov, Almas</a>,  <em>Taunyazov, Tasbolat</em>, Omarali, Bukeikhan, Nurbayeva, Aigerim, Kim, Anton, Bukeyev, Askhat,  and Rubagotti, Matteo </div> <div class="periodical"> <em>Sensors</em> 2020 </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://www.mdpi.com/1424-8220/20/11/3082" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">HTML</a> </div> <div class="abstract hidden"> <p>To extend the choice of inertial motion-tracking systems freely available to researchers and educators, this paper presents an alternative open-source design of a wearable 7-DOF wireless human arm motion-tracking system. Unlike traditional inertial motion-capture systems, the presented system employs a hybrid combination of two inertial measurement units and one potentiometer for tracking a single arm. The sequence of three design phases described in the paper demonstrates how the general concept of a portable human arm motion-tracking system was transformed into an actual prototype, by employing a modular approach with independent wireless data transmission to a control PC for signal processing and visualization. Experimental results, together with an application case study on real-time robot-manipulator teleoperation, confirm the applicability of the developed arm motion-tracking system for facilitating robotics research. The presented arm-tracking system also has potential to be employed in mechatronic system design education and related research activities. The system CAD design models and program codes are publicly available online and can be used by robotics researchers and educators as a design platform to build their own arm-tracking solutions for research and educational purposes.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">shintemirov2020open</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{An open-source 7-DOF wireless human arm motion-tracking system for use in robotics research}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Shintemirov, Almas and Taunyazov, Tasbolat and Omarali, Bukeikhan and Nurbayeva, Aigerim and Kim, Anton and Bukeyev, Askhat and Rubagotti, Matteo}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{Sensors}</span><span class="p">,</span>
  <span class="na">volume</span> <span class="p">=</span> <span class="s">{20}</span><span class="p">,</span>
  <span class="na">number</span> <span class="p">=</span> <span class="s">{11}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{3082}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2020}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{Multidisciplinary Digital Publishing Institute}</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"> <abbr class="badge">IROS</abbr> </div> <div id="gu2020tactilesgnet" class="col-sm-8"> <div class="title">Tactilesgnet: A spiking graph neural network for event-based tactile object recognition</div> <div class="author"> Gu, Fuqiang, Sng, Weicong,  <em>Taunyazov, Tasbolat</em>,  and <a href="https://haroldsoh.github.io/" target="_blank" rel="noopener noreferrer">Soh, Harold</a> </div> <div class="periodical"> <em>In IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)</em> 2020 </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="http://arxiv.org/abs/2008.08046" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">arXiv</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://github.com/clear-nus/TactileSGNet" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Code</a> </div> <div class="abstract hidden"> <p>Tactile perception is crucial for a variety of robot tasks including grasping and in-hand manipulation. New advances in flexible, event-driven, electronic skins may soon endow robots with touch perception capabilities similar to humans. These electronic skins respond asynchronously to changes (e.g., in pressure, temperature), and can be laid out irregularly on the robot’s body or end-effector. However, these unique features may render current deep learning approaches such as convolutional feature extractors unsuitable for tactile learning. In this paper, we propose a novel spiking graph neural network for event-based tactile object recognition. To make use of local connectivity of taxels, we present several methods for organizing the tactile data in a graph structure. Based on the constructed graphs, we develop a spiking graph convolutional network. The event-driven nature of spiking neural network makes it arguably more suitable for processing the event-based data. Experimental results on two tactile datasets show that the proposed method outperforms other state-of-the-art spiking methods, achieving high accuracies of approximately 90% when classifying a variety of different household objects.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">gu2020tactilesgnet</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Tactilesgnet: A spiking graph neural network for event-based tactile object recognition}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Gu, Fuqiang and Sng, Weicong and Taunyazov, Tasbolat and Soh, Harold}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{9876--9882}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2020}</span><span class="p">,</span>
  <span class="na">organization</span> <span class="p">=</span> <span class="s">{IEEE}</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"> <abbr class="badge">IROS</abbr> </div> <div id="gao2020supervised" class="col-sm-8"> <div class="title">Supervised autoencoder joint learning on heterogeneous tactile sensory data: Improving material classification performance</div> <div class="author"> Gao, Ruihan,  <em>Taunyazov, Tasbolat</em>, Lin, Zhiping,  and <a href="https://yan-wu.com/" target="_blank" rel="noopener noreferrer">Wu, Yan</a> </div> <div class="periodical"> <em>In IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)</em> 2020 </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://ieeexplore.ieee.org/document/9341111" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">HTML</a> <a href="https://github.com/clear-nus/ext-sense.git" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Code</a> </div> <div class="abstract hidden"> <p>The sense of touch is an essential sensing modality for a robot to interact with the environment as it provides rich and multimodal sensory information upon contact. It enriches the perceptual understanding of the environment and closes the loop for action generation. One fundamental area of perception that touch dominates over other sensing modalities, is the understanding of the materials that it interacts with, for example, glass versus plastic. However, unlike the senses of vision and audition which have standardized data format, the format for tactile data is vastly dictated by the sensor manufacturer, which makes it difficult for large-scale learning on data collected from heterogeneous sensors, limiting the usefulness of publicly available tactile datasets. This paper investigates the joint learnability of data collected from two tactile sensors performing a touch sequence on some common materials. We propose a supervised recurrent autoencoder framework to perform joint material classification task to improve the training effectiveness. The framework is implemented and tested on the two sets of tactile data collected in sliding motion on 20 material textures using the iCub RoboSkin tactile sensors and the SynTouch BioTac sensor respectively. Our results show that the learning efficiency and accuracy improve for both datasets through the joint learning as compared to independent dataset training. This suggests the usefulness for large-scale open tactile datasets sharing with different sensors.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">gao2020supervised</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Gao, Ruihan and Taunyazov, Tasbolat and Lin, Zhiping and Wu, Yan}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{10907--10913}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2020}</span><span class="p">,</span>
  <span class="na">organization</span> <span class="p">=</span> <span class="s">{IEEE}</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"> <abbr class="badge">IROS</abbr> </div> <div id="taunyazov2020fast" class="col-sm-8"> <div class="title">Fast texture classification using tactile neural coding and spiking neural network</div> <div class="author"> <em>Taunyazov, Tasbolat</em>, Chua, Yansong, Gao, Ruihan, <a href="https://haroldsoh.github.io/" target="_blank" rel="noopener noreferrer">Soh, Harold</a>,  and <a href="https://yan-wu.com/" target="_blank" rel="noopener noreferrer">Wu, Yan</a> </div> <div class="periodical"> <em>In IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)</em> 2020 </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://ieeexplore.ieee.org/document/9340693" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">HTML</a> <a href="/assets/pdf/taunyazov2020fast.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> <a href="/assets/pdf/IROS2020.pdf" class="btn btn-sm z-depth-0" role="button">Slides</a> </div> <div class="abstract hidden"> <p>Touch is arguably the most important sensing modality in physical interactions. However, tactile sensing has been largely under-explored in robotics applications owing to the complexity in making perceptual inferences until the recent advancements in machine learning or deep learning in particular. Touch perception is strongly influenced by both its temporal dimension similar to audition and its spatial dimension similar to vision. While spatial cues can be learned episodically, temporal cues compete against the system’s re- sponse/reaction time to provide accurate inferences. In this paper, we propose a fast tactile-based texture classification framework which makes use of the spiking neural network to learn from the neural coding of the conventional tactile sensor readings. The framework is implemented and tested on two independent tactile datasets collected in sliding motion on 20 material textures. Our results show that the framework is able to make much more accurate inferences ahead of time as compared to that by the state-of-the-art learning approaches.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">taunyazov2020fast</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Fast texture classification using tactile neural coding and spiking neural network}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Taunyazov, Tasbolat and Chua, Yansong and Gao, Ruihan and Soh, Harold and Wu, Yan}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{9890--9895}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2020}</span><span class="p">,</span>
  <span class="na">organization</span> <span class="p">=</span> <span class="s">{IEEE}</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> </ol> <h2 class="year">2019</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col-sm-2 abbr"> <abbr class="journal badge">RA-L</abbr> <span class="award badge">presented @ R:SS</span> </div> <div id="rubagotti2019semi" class="col-sm-8"> <div class="title">Semi-autonomous robot teleoperation with obstacle avoidance via model predictive control</div> <div class="author"> Rubagotti, Matteo,  <em>Taunyazov, Tasbolat</em>, Omarali, Bukeikhan,  and <a href="https://www.alaris.kz/" target="_blank" rel="noopener noreferrer">Shintemirov, Almas</a> </div> <div class="periodical"> <em>IEEE Robotics and Automation Letters</em> 2019 </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://ieeexplore.ieee.org/document/8718327" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">HTML</a> <a href="/assets/pdf/teleoperation_rss2020.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> </div> <div class="abstract hidden"> <p>This paper proposes a model predictive control ap- proach for semi-autonomous teleoperation of robot manipulators: the focus is on avoiding obstacles with the whole robot frame, while exploiting predictions of the operator’s motion. The hand pose of the human operator provides the reference for the end effector, and the robot motion is continuously replanned in real time, satisfying several constraints. An experimental case study is described regarding the design and testing of the described framework on a UR5 manipulator: the experimental results con- firm the suitability of the proposed method for semi-autonomous teleoperation, both in terms of performance (tracking capability and constraint satisfaction) and computational complexity (the control law is calculated well within the sampling interval).</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">rubagotti2019semi</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Semi-autonomous robot teleoperation with obstacle avoidance via model predictive control}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Rubagotti, Matteo and Taunyazov, Tasbolat and Omarali, Bukeikhan and Shintemirov, Almas}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{IEEE Robotics and Automation Letters}</span><span class="p">,</span>
  <span class="na">volume</span> <span class="p">=</span> <span class="s">{4}</span><span class="p">,</span>
  <span class="na">number</span> <span class="p">=</span> <span class="s">{3}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{2746--2753}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2019}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{IEEE}</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"> <abbr class="badge">ICRA</abbr> </div> <div id="taunyazov2019towards" class="col-sm-8"> <div class="title">Towards effective tactile identification of textures using a hybrid touch approach</div> <div class="author"> <em>Taunyazov, Tasbolat</em>, Koh, Hui Fang, <a href="https://yan-wu.com/" target="_blank" rel="noopener noreferrer">Wu, Yan</a>, Cai, Caixia,  and <a href="https://haroldsoh.github.io/" target="_blank" rel="noopener noreferrer">Soh, Harold</a> </div> <div class="periodical"> <em>In International Conference on Robotics and Automation (ICRA)</em> 2019 </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="/assets/pdf/taunyanovicra19.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> <a href="https://github.com/clear-nus/TactileLearning" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Code</a> <a href="/assets/pdf/poster_ICRA2019.pdf" class="btn btn-sm z-depth-0" role="button">Poster</a> <a href="/assets/pdf/presentation_ICRA2019.pdf" class="btn btn-sm z-depth-0" role="button">Slides</a> </div> <div class="abstract hidden"> <p>The sense of touch is arguably the first human sense to develop. Empowering robots with the sense of touch may augment their understanding of interacted objects and the environment beyond standard sensory modalities (e.g., vision). This paper investigates the effect of hybridizing touch and sliding movements for tactile-based texture classification. We develop three machine-learning methods within a framework to discriminate between surface textures; the first two methods use hand-engineered features, whilst the third leverages convo- lutional and recurrent neural network layers to learn feature representations from raw data. To compare these methods, we constructed a dataset comprising tactile data from 23 textures gathered using the iCub platform under a loosely constrained setup, i.e., with nonlinear motion. In line with findings from neuroscience, our experiments show that a good initial estimate can be obtained via touch data, which can be further refined via sliding; combining both touch and sliding data results in 98% classification accuracy over unseen test data.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">taunyazov2019towards</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Towards effective tactile identification of textures using a hybrid touch approach}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Taunyazov, Tasbolat and Koh, Hui Fang and Wu, Yan and Cai, Caixia and Soh, Harold}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{International Conference on Robotics and Automation (ICRA)}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{4269--4275}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2019}</span><span class="p">,</span>
  <span class="na">organization</span> <span class="p">=</span> <span class="s">{IEEE}</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> </ol> <h2 class="year">2017</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col-sm-2 abbr"> <abbr class="journal badge">IEEE ASME ToM</abbr> </div> <div id="taunyazov2017constrained" class="col-sm-8"> <div class="title">Constrained orientation control of a spherical parallel manipulator via online convex optimization</div> <div class="author"> <em>Taunyazov, Tasbolat</em>, Rubagotti, Matteo,  and <a href="https://www.alaris.kz/" target="_blank" rel="noopener noreferrer">Shintemirov, Almas</a> </div> <div class="periodical"> <em>IEEE/ASME Transactions on Mechatronics</em> 2017 </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://ieeexplore.ieee.org/document/8113518" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">HTML</a> <a href="/assets/pdf/ttaunyazov_TMECH2774245.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> </div> <div class="abstract hidden"> <p>This paper introduces a new framework for the closed-loop orientation control of spherical parallel manipulators (SPMs) based on the online solution of a convex optimization problem. The aim of solving a constrained optimization problem is to define a reference position for the SPM that remains as close as possible to the ideal reference (i.e., the one for which the top mobile platform has the desired orientation), at the same time keeping the SPM within the set of configurations in which collisions between links and singular configurations are avoided (the so-called feasible workspace). The proposed approach relies on a recently introduced method for obtaining unique inverse kinematics for SPMs, and on a newly proposed method for generating an approximation of the feasible workspace suitable for fast online optimization. The proposed control scheme is ex- perimentally tested on an Agile Wrist SPM prototype, confirming the performance expected from the theoretical formulation.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">taunyazov2017constrained</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Constrained orientation control of a spherical parallel manipulator via online convex optimization}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Taunyazov, Tasbolat and Rubagotti, Matteo and Shintemirov, Almas}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{IEEE/ASME Transactions on Mechatronics}</span><span class="p">,</span>
  <span class="na">volume</span> <span class="p">=</span> <span class="s">{23}</span><span class="p">,</span>
  <span class="na">number</span> <span class="p">=</span> <span class="s">{1}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{252--261}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2017}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{IEEE}</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"> <abbr class="badge">HRI</abbr> <span class="award badge">late-breaking report</span> </div> <div id="omarali2017real" class="col-sm-8"> <div class="title">Real-time predictive control of an ur5 robotic arm through human upper limb motion tracking</div> <div class="author"> Omarali, Bukeikhan,  <em>Taunyazov, Tasbolat</em>, Bukeyev, Askhat,  and <a href="https://www.alaris.kz/" target="_blank" rel="noopener noreferrer">Shintemirov, Almas</a> </div> <div class="periodical"> <em>In Proceedings of the Companion of the 2017 ACM/IEEE International Conference on Human-Robot Interaction</em> 2017 </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="/assets/pdf/lbr1899_Omarali.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> <a href="/assets/pdf/omarali_hri2017_poster.pdf" class="btn btn-sm z-depth-0" role="button">Poster</a> </div> <div class="abstract hidden"> <p>This paper reports the authors’ initial results on develop- ing a real-time teleoperation system for an Universal Robots robotic arm through human motion capture with a visualiza- tion utility built on the Blender Game Engine open-source platform. A linear explicit model predictive robot controller (EMPC) is implemented for online generation of optimal robot trajectories matching operator’s wrist position and orientation, whilst adhering to the robot’s constraints. The EMPC proved to be superior to open-loop and naive PID controllers in terms of accuracy and safety.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">omarali2017real</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Real-time predictive control of an ur5 robotic arm through human upper limb motion tracking}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Omarali, Bukeikhan and Taunyazov, Tasbolat and Bukeyev, Askhat and Shintemirov, Almas}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Proceedings of the Companion of the 2017 ACM/IEEE International Conference on Human-Robot Interaction}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{237--238}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2017}</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> </ol> <h2 class="year">2016</h2> <ol class="bibliography"><li> <div class="row"> <div class="col-sm-2 abbr"> <abbr class="badge">BioRob</abbr> </div> <div id="taunyazov2016novel" class="col-sm-8"> <div class="title">A novel low-cost 4-DOF wireless human arm motion tracker</div> <div class="author"> <em>Taunyazov, Tasbolat</em>, Omarali, Bukeikhan,  and <a href="https://www.alaris.kz/" target="_blank" rel="noopener noreferrer">Shintemirov, Almas</a> </div> <div class="periodical"> <em>In 6th IEEE International Conference on Biomedical Robotics and Biomechatronics (BioRob)</em> 2016 </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://ieeexplore.ieee.org/document/7523615?arnumber=7523615" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">HTML</a> <a href="/assets/pdf/taunyazov2016.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> </div> <div class="abstract hidden"> <p>A human arm can be described as a five degrees- of-freedom (DOF) serial manipulator. The fifth degree - rotation around the forearm axis only contributes to the wrist orienta- tion. Hence, if it is ignored the elbow and wrist joint positions can be tracked using an upper arm orientation and the elbow joint angle. The paper presents a novel low-cost design of a 4- DOF human arm wearable tracker system for wireless dynamic tracking of upper limb position and orientation. The proposed design utilizes a single inertial measurement unit coupled with an Unscented Kalman filter for the upper arm orientation quaternion and a potentiometer sensor for elbow joint angle estimations. The presented arm tracker prototype implements wireless communication with the control PC for sensor data transmission and real-time visualization using a Blender open source 3D computer graphics software and was verified with an Xsens MVN motion tracking system.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">taunyazov2016novel</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{A novel low-cost 4-DOF wireless human arm motion tracker}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Taunyazov, Tasbolat and Omarali, Bukeikhan and Shintemirov, Almas}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{6th IEEE International Conference on Biomedical Robotics and Biomechatronics (BioRob)}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{157--162}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2016}</span><span class="p">,</span>
  <span class="na">organization</span> <span class="p">=</span> <span class="s">{IEEE}</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li></ol> <h2 class="year">2015</h2> <ol class="bibliography"><li> <div class="row"> <div class="col-sm-2 abbr"> <abbr class="badge">SSI</abbr> </div> <div id="omarali2015system" class="col-sm-8"> <div class="title">System integration of a solar sensor and a spherical parallel manipulator for a 3-axis solar tracker platform design</div> <div class="author"> Omarali, Bukeikhan,  <em>Taunyazov, Tasbolat</em>, Nyetkaliyev, Aibek,  and <a href="https://www.alaris.kz/" target="_blank" rel="noopener noreferrer">Shintemirov, Almas</a> </div> <div class="periodical"> <em>In IEEE/SICE International Symposium on System Integration (SII)</em> 2015 </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://ieeexplore.ieee.org/document/7405038" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">HTML</a> <a href="/assets/pdf/omarali2015.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> </div> <div class="abstract hidden"> <p>This paper presents the authors’ ongoing work on designing a novel 3-axis solar tracker platform utilizing a 3-DOF spherical parallel manipulator with revolute joints and a solar sensor. The selected solar sensor and the SPM conﬁguration are described in details. A novel approach is proposed for the SPM platform orientation estimation based on solar sensor measurements employing trigonometric identities and quaternion rotation representation. The proposed approach is experimentally veriﬁed using a SPM 3D-printed prototype equipped with a solar and an orientation sensors. It is assumed that the proposed concept for a novel 3-axis solar tracker platform can further applied to design novel mobile solar tracking systems.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">omarali2015system</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{System integration of a solar sensor and a spherical parallel manipulator for a 3-axis solar tracker platform design}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Omarali, Bukeikhan and Taunyazov, Tasbolat and Nyetkaliyev, Aibek and Shintemirov, Almas}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{IEEE/SICE International Symposium on System Integration (SII)}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{546--551}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2015}</span><span class="p">,</span>
  <span class="na">organization</span> <span class="p">=</span> <span class="s">{IEEE}</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li></ol> </div> </article> </div> </div> <footer class="fixed-bottom"> <div class="container mt-0"> © Copyright 2025 Tasbolat Taunyazov. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/js/bootstrap.bundle.min.js" integrity="sha256-fgLAgv7fyCGopR/gBNq2iW3ZKIdqIcyshnUULC4vex8=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.6/dist/medium-zoom.min.js" integrity="sha256-EdPgYcPk/IIrw7FYeuJQexva49pVRZNmt3LculEr7zM=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js"></script> <script defer src="/assets/js/common.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script async src="https://www.googletagmanager.com/gtag/js?id=G-CM3L1EM35M"></script> <script>function gtag(){dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],gtag("js",new Date),gtag("config","G-CM3L1EM35M");</script> </body> </html>